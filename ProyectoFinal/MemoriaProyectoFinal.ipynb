{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto final: Predicción de bancarrota {-}\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción y preparación inicial {-}\n",
    "Para este proyecto hemos usado un set de datos que contiene distintas características de miles de empresas acompañadas de una columna que indica si esas empresas acabaron o no en bancarrota.\n",
    "\n",
    "Los datos en cuestión han sido extraídos de https://www.kaggle.com/fedesoriano/company-bankruptcy-prediction\n",
    "\n",
    "Sin embargo, este dataset tiene una peculiaridad: solo cuenta con 220 ejemplos de empresas caídas en bancarrota y unos 6600 que no lo hicieron, por lo que está muy desiquilibrado. Esto puede supone un problema bastante grande para aplicar métodos de aprendizaje automático. Es por ello que hemos recurrido a la técnica SMOTE (Synthetic Minority Oversampling Technique) para poder hacer un estudio más certero de los datos.\n",
    "\n",
    "En este proyecto aplicaremos tres métodos de aprendizaje automático: regresión logística multiclase, redes neuronales y SVM, y compararemos los resultados obtenidos para el dataset original, un dataset equilibrado manualmente (eliminando gran parte de los ejemplos mayoritarios para que haya igualdad en número) y un dataset equilibrado mediante SMOTE.\n",
    "\n",
    "Para programar el proyecto se han utilizado las siguientes librerías y métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.parsers import read_csv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.optimize as opt\n",
    "import operator\n",
    "import checkNNGradients as cnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.special as special\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran tres gráficas que muestran la distribución de los datos en cada dataset que hemos utilizado. Los datos representados en verde son aquellos de empresas que no cayeron en bancarrota, y los rojos son los datos de empresas que sí lo hicieron.\n",
    "\n",
    "Esta es la gráfica de los datos originales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos originales](Resultados/DataAllExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la gráfica con los datos equilibrados a mano:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos originales](Resultados/DataReducedExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último la gráfica con los datos equilibrados con SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos SMOTE](Resultados/DataSMOTEBalanced.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrado de datos con SMOTE\n",
    "\n",
    "Hemos hecho uso de la librería imblearn para aplicar SMOTE a nuestro dataset. Hemos inicializado un obeto sm y luego lo hemos usado para equilibrar el tamaño de las muestras en X e y. Antes de eso hemos tenido que normalizar los datos, para lo cual hemos tenido que separar las columnas de números que están entre 0 y 1 y las que no, para que los números resultantes no estén también desequilibrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fraction_valued_columns(df):\n",
    "    my_columns  = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].max()<=1) & (df[col].min() >= 0):\n",
    "            my_columns.append(col)\n",
    "    return(my_columns)\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "cname = 'Bankrupt?'\n",
    "\n",
    "fractional_columns = get_fraction_valued_columns(df=df.drop([cname],axis=1))\n",
    "non_fractional_columns = df.drop([cname],axis=1).columns.difference(fractional_columns)\n",
    "norm = preprocessing.normalize(df[non_fractional_columns])\n",
    "normalized_df = pd.DataFrame(norm, columns=df[non_fractional_columns].columns)\n",
    "\n",
    "scaled_data = pd.concat([df.drop(non_fractional_columns,axis=1),normalized_df],axis = 1)\n",
    "\n",
    "X = scaled_data.drop(cname, axis = 1)\n",
    "y = scaled_data[cname]\n",
    "\n",
    "sm = SMOTE(random_state=123)\n",
    "X_sm , y_sm = sm.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística multiclase {-}\n",
    "\n",
    "Para aplicar este método hemos hecho uso del siguiente método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test, fileName):\n",
    "\n",
    "\tlambdas = np.linspace(1, 3, 20)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor i in lambdas:\n",
    "\t\tclasificadores = oneVsAll(X_train, y_train, 2, i)\n",
    "\t\ty_pred, acc = prediccion(X_test, y_test, clasificadores)\n",
    "\t\taccuracy.append(acc)\n",
    "\t\tprint(acc)\n",
    "\t\tcm = confusion_matrix(y_test, y_pred)\n",
    "\t\tplt.figure()\n",
    "\t\tfig = sns.heatmap(cm, annot=True,fmt=\"\",cmap='Blues').get_figure()\n",
    "\t\tfig.savefig(fileName + \"iter\" + str(i) + \".png\", dpi=400)\n",
    "\n",
    "\tplt.figure()\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"lambdaAccuracyLogistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que utiliza los siguientes métodos auxiliares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_etiquetas, reg):\n",
    "\tclasificadores = np.zeros(shape=(num_etiquetas, X.shape[1]))\n",
    "\t\n",
    "\tfor i in range(num_etiquetas):\n",
    "\t\tfiltrados = (y==i) * 1\n",
    "\t\tthetas = np.zeros(np.shape(X)[1])\n",
    "\t\tclasificadores[i] = opt.fmin_tnc(func=costeReg, x0=thetas, fprime=gradienteReg, args=(X, filtrados, reg), messages=0)[0]\n",
    "\t\t\n",
    "\treturn clasificadores\n",
    "\n",
    "def prediccion(X, Y, clasificadores):\n",
    "\tpredicciones = {}\n",
    "\tY_pred = []\n",
    "\tfor imagen in range(np.shape(X)[0]):\n",
    "\t\tfor i in range(clasificadores.shape[0]):\n",
    "\t\t\ttheta_opt = clasificadores[i]\n",
    "\t\t\tprediccion = sigmoide(\n",
    "\t\t\t\tnp.matmul(np.transpose(theta_opt), X[imagen]))\n",
    "\n",
    "\t\t\tpredicciones[i] = prediccion\n",
    "\t\tY_pred.append(max(predicciones.items(), key=operator.itemgetter(1))[0])\n",
    "\treturn Y_pred, np.sum((Y == np.array(Y_pred)))/np.shape(X)[0] * 100\n",
    "\n",
    "def costeReg(thetas, x, y, lamb):\n",
    "\tsigXT = sigmoide(np.dot(x, thetas))\n",
    "\ta1 = (-1/np.shape(x)[0])\n",
    "\ta2 = np.dot(np.log(sigXT), y)\n",
    "\ta3 = np.dot(np.log(1-sigXT+1e-6), 1-y)\n",
    "\ta = a1 * (a2+a3)\n",
    "\tb = ((lamb/(2*np.shape(x)[0])) * np.sum(thetas ** 2))\n",
    "\treturn a + b\n",
    "\n",
    "def gradienteReg(thetas, x, y, lamb):\n",
    "\tsigXT = sigmoide(np.matmul(x, thetas))\n",
    "\ta = ((1/np.shape(x)[0]) * np.matmul(np.transpose(x), (sigXT - y))) + ((lamb/np.shape(x)[0]) * thetas)\n",
    "\treturn a\n",
    "\n",
    "def sigmoide(x):\n",
    "\treturn special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son los siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset original:\n",
    "\n",
    "![Gráfica con datos originales](Resultados/OriginalLambdaAccuracyLogistic.png)\n",
    "![Gráfica con datos originales](Resultados/OriginalLogistic.png)\n",
    "\n",
    "Podemos observar que con este entrenamiento siempre se predice que la compañía no está en bancarrota, ya que al haber muchos más ejemplos de esto que del caso contrario, las predicciones están muy sesgadas hacia el caso negativo. De hecho, si utilizáramos una porción del dataset original como casos de prueba, obtendríamos una precisión muy alta, de alrededor del 97%, pero esto se debe a que siempre se predice el caso negativo y el dataset consta de 97% casos negativos. Para resaltar que la predicción no es buena, se han elegido casos de prueba que sean 50% positivos y 50% negativos.\n",
    "El hecho de que la precisión no cambie respecto a lambda posiblemente se debe a que el sesgo es tan grande que siempre se predice el caso negativo, además de que hay muy pocos casos de prueba.\n",
    "\n",
    "Dataset balanceado a mano:\n",
    "\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutLambdaAccuracyLogistic.png)\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutLogistic.png)\n",
    "\n",
    "La precisión ha mejorado bastante, estando ahora alrededor del 66% y por la matriz de confusión podemos ver que ahora a veces predice que sí y a veces que no. Sin embargo, sigue sin ser una probabilidad muy alta ya que, por cortar el dataset para que haya la misma proporción de 0s y 1s, tan solo hay unos 400 ejemplos de entrenamiento, que no son suficientes para alcanzar una mayor precisión. También podemos observar que la precisión no cambia respecto a lambda, probablemente debido al pequeño número de casos de prueba, ya que con tan pocos, la precisión subiría por más de 1% por cada caso acertado, y estos valores de lambda no parecen ser suficientes para crear un cambio tan grande.\n",
    "\n",
    "Dataset balanceado con SMOTE:\n",
    "\n",
    "![Gráfica con datos SMOTE](Resultados/SmoteLambdaAccuracyLogistic.png)\n",
    "![Gráfica con datos SMOTE](Resultados/SmoteLogistic.png)\n",
    "\n",
    "Finalmente, los resultados con el dataset balanceado con SMOTE son mucho mejores. La precisión llega casi a 90% gracias a contar ahora con muchos más ejemplos para el entrenamiento, y además, como tiene más casos de prueba, el efecto de lambda sí es apreciable. En general vemos que lambdas más pequeñas dan mayor precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales {-}\n",
    "\n",
    "Hemos aplicado el método de la red neuronal con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, fileName):\n",
    "\n",
    "\tm = len(y)\n",
    "\tinput_size = X.shape[1]\n",
    "\tnum_labels = 10\n",
    "\tnum_ocultas = 25\n",
    "\n",
    "\ty_onehot = np.zeros((m, num_labels))\n",
    "\n",
    "\tfor i in range(m):\n",
    "\t\ty_onehot[i][int(y[i])] = 1\n",
    "\n",
    "\ttheta1 = random_weights(num_ocultas, input_size + 1)\n",
    "\ttheta2 = random_weights(num_labels, num_ocultas +1)\n",
    "\tparams_rn = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "\treg_param = 1\n",
    "\n",
    "\tcost, grad = backprop(params_rn, input_size, num_ocultas,\n",
    "\t\t\t\t\t\tnum_labels, X, y_onehot, reg_param)\n",
    "\ta = cnn.checkNNGradients(backprop, 1)\n",
    "\tprint(a)\n",
    "\n",
    "\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "\ttheta1_opt = np.reshape(\n",
    "\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\ttheta2_opt = np.reshape(\n",
    "\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\n",
    "\tprint(\"El porcentaje de acierto del modelo es: {}%\".format(\n",
    "\t\tnp.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "\tlambdas = np.linspace(0, 1, 10)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor lamb in lambdas:\n",
    "\t\tprint(\"Voy por \", lamb, \" de \", len(lambdas))\n",
    "\t\treg_param = lamb\n",
    "\t\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "\t\ttheta1_opt = np.reshape(\n",
    "\t\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\t\ttheta2_opt = np.reshape(\n",
    "\t\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\t\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\t\taccuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"lambdaAccuracy\")\n",
    "\n",
    "\tlambdas = np.linspace(10, 70, 7)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor lamb in lambdas:\n",
    "\t\treg_param = lamb\n",
    "\t\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': int(lamb)})\n",
    "\n",
    "\t\ttheta1_opt = np.reshape(\n",
    "\t\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\t\ttheta2_opt = np.reshape(\n",
    "\t\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\t\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\t\taccuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\tplt.figure()\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"iterationAccuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que utiliza los siguientes métodos auxiliares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_prima(x):\n",
    "\treturn sigmoide(x) / (1 - sigmoide(x))\n",
    "\n",
    "def propagar(X, theta1, theta2):\n",
    "\tm = np.shape(X)[0]\n",
    "\n",
    "\ta1 = np.hstack([np.ones([m, 1]), X])\n",
    "\tz2 = np.dot(a1, theta1.T)\n",
    "\ta2 = np.hstack([np.ones([m, 1]), sigmoide(z2)])\n",
    "\tz3 = np.dot(a2, theta2.T)\n",
    "\ta3 = sigmoide(z3)\n",
    "\treturn a1, a2, a3\n",
    "\n",
    "\n",
    "def coste_neuronal(X, y, theta1, theta2, reg):\n",
    "\ta1, a2, h = propagar(X, theta1, theta2)\n",
    "\tm = X.shape[0]\n",
    "\n",
    "\tJ = 0\n",
    "\tfor i in range(m):\n",
    "\t\tJ += np.sum(-y[i]*np.log(h[i]) - (1 - y[i])*np.log(1-h[i]+1e-9))\n",
    "\tJ = J/m\n",
    "\n",
    "\tsum_theta1 = np.sum(np.square(theta1[:, 1:]))\n",
    "\tsum_theta2 = np.sum(np.square(theta2[:, 1:]))\n",
    "\n",
    "\treg_term = (sum_theta1 + sum_theta2) * reg / (2*m)\n",
    "\n",
    "\treturn J + reg_term\n",
    "\n",
    "\n",
    "def gradiente(X, y, Theta1, Theta2, reg):\n",
    "\tm = X.shape[0]\n",
    "\n",
    "\tdelta1 = np.zeros(Theta1.shape)\n",
    "\tdelta2 = np.zeros(Theta2.shape)\n",
    "\n",
    "\ta1, a2, h = propagar(X, Theta1, Theta2)\n",
    "\n",
    "\tfor t in range(m):\n",
    "\t\ta1t = a1[t, :]\n",
    "\t\ta2t = a2[t, :]\n",
    "\t\tht = h[t, :]\n",
    "\t\tyt = y[t]\n",
    "\t\td3t = ht - yt\n",
    "\t\td2t = np.dot(Theta2.T, d3t) * (a2t * (1 - a2t))\n",
    "\n",
    "\t\tdelta1 = delta1 + np.dot(d2t[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "\t\tdelta1[:, 1:] += Theta1[:, 1:] * reg/m\n",
    "\t\tdelta2 = delta2 + np.dot(d3t[:, np.newaxis], a2t[np.newaxis, :])\n",
    "\t\tdelta2[:, 1:] += Theta2[:, 1:] * reg/m\n",
    "\n",
    "\treturn np.concatenate((np.ravel(delta1/m), np.ravel(delta2/m)))\n",
    "\n",
    "\n",
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "\tTheta1 = np.reshape(\n",
    "\t\tparams_rn[:num_ocultas * (num_entradas + 1)], (num_ocultas, (num_entradas + 1)))\n",
    "\tTheta2 = np.reshape(\n",
    "\t\tparams_rn[num_ocultas * (num_entradas + 1):], (num_etiquetas, (num_ocultas+1)))\n",
    "\treturn coste_neuronal(X, y, Theta1, Theta2, reg), gradiente(X, y, Theta1, Theta2, reg)\n",
    "\n",
    "\n",
    "def random_weights(L_in, L_out):\n",
    "\tepsilon = np.sqrt(6)/np.sqrt(L_in + L_out)\n",
    "\treturn np.random.random((L_in, L_out)) * epsilon - epsilon/2\n",
    "\n",
    "\n",
    "def predict_nn(X, h):\n",
    "\treturn [(np.argmax(h[image])) for image in range(X.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset original:\n",
    "\n",
    "![Gráfica con datos originales](Resultados/OriginalNeural.png)\n",
    "![Gráfica con datos originales](Resultados/OriginalIterationAccuracyNeural.png)\n",
    "\n",
    "Aquí se da el mismo caso que en la regresión logística. Sin embargo, al no ser tan fácil cambiar a nuestro gusto la cantidad de datos de prueba de cada clase, podemos observar lo que pasa si todo se deja tal cual. La proporción de aciertos es muy alta, pero esto se debe únicamente a que hay muchos más ejemplos de empresas que no han caído en bancarrota, y nuestro modelo está tan sesgado que siempre dice que la empresa no está en bancarrota y acierta la mayoría de las veces.\n",
    "\n",
    "Dataset balanceado a mano:\n",
    "\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutNeural.png)\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutIterationAccuracyNeural.png)\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutNeuralBestLambda.png)\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutNeuralBestIter.png)\n",
    "\n",
    "Al igual que ocurrió con la regresión logística, el dataset balanceado predice realmente en vez de decir siempre que no, pero no llega a ser muy preciso. Un gráfico muestra la mejoría en precisión según el número de iteraciones que no consigue llegar a 70% de precisión. Las otras matrices de confusión son las de la mejor lambda y el mejor número de iteraciones, respectivamente.\n",
    "\n",
    "Dataset balanceado con SMOTE:\n",
    "\n",
    "![Gráfica con datos SMOTE](Resultados/SmoteNeural.png)\n",
    "\n",
    "Al igual que pasó antes, los datos balanceados con Smote dan resultados mucho mejores que incluso rozan el 90% de precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine {-}\n",
    "\n",
    "Para aplicar la técnica de las SVM, hemos hecho uso del siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X, y, fileName):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) \n",
    "\t\n",
    "\tvalues = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]\n",
    "\tn = len(values)\n",
    "\tscores = np.zeros((n, n))\n",
    "\n",
    "\tstartTime = time.process_time()\n",
    "\n",
    "\tfor i in range(n):\n",
    "\t\tC = values[i]\n",
    "\t\tprint(\"Voy por el i \", i, \"de \" , n) \n",
    "\t\tfor j in range(n):\n",
    "\t\t\tprint(\"Voy por el j \", j, \"de \" , n) \n",
    "\t\t\tsigma = values[j]\n",
    "\t\t\tsvm = SVC(kernel='rbf', C = C, gamma= 1 / (2 * sigma **2))\n",
    "\t\t\tsvm.fit(X_train, y_train)\n",
    "\t\t\tscores[i, j] = svm.score(X_val, y_val)\n",
    "\n",
    "\tprint(\"Error mínimo: {}\".format(1 - scores.max())) \n",
    "\tC_opt = values[scores.argmax()//n]\n",
    "\tsigma_opt = values[scores.argmax()%n]\n",
    "\tprint(\"C óptimo: {}, sigma óptimo: {}\".format(C_opt, sigma_opt))\n",
    "\n",
    "\tsvm = SVC(kernel='rbf', C= C_opt, gamma=1 / (2 * sigma_opt)**2)\n",
    "\tsvm.fit(X_train, y_train)\n",
    "\ty_pred = svm.predict(X_test)\n",
    "\tendTime = time.process_time()\n",
    "\n",
    "\tscore = svm.score(X_test, y_test)\n",
    "\ttotalTime = endTime - startTime\n",
    "\n",
    "\tprint('Precisión: {:.3f}%'.format(score*100))\n",
    "\tprint('Tiempo de ejecución: {}'.format(totalTime))\n",
    "\tprint('Matriz de confusión: ')\n",
    "\tcm = confusion_matrix(y_test, y_pred)\n",
    "\tfig = sns.heatmap(cm, annot=True,fmt=\"\",cmap='Blues').get_figure()\n",
    "\tfig.savefig(fileName, dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset original:\n",
    "\n",
    "![Gráfica con datos originales](Resultados/OriginalSVM.png)\n",
    "\n",
    "Pasa lo mismo que en los dos casos anteriores. El modelo ha recibido tantos datos de empresas que no están en bancarrota que siempre clasifica a todos los ejemplos como 'no en bancarrota' (0).\n",
    "\n",
    "Dataset balanceado a mano:\n",
    "\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutSVM.png)\n",
    "\n",
    "Este es un caso curioso porque no actúa de forma similar a los anteriores casos de estudio con este dataset, sino que también elige siempre 0. Esto se debe a que no se han normalizado los datos y por lo tanto los cálculos no son correctos. A continuación se muestra el gráfico resultante cuando los datos sí están normalizados.\n",
    "\n",
    "![Gráfica con datos equilibrados a mano](Resultados/CutSVMGood.png)\n",
    "\n",
    "Tras normalizar, incluso con solo 440 ejemplos conseguimos una precisión de alrededor del 84%. Esto demuestra la importancia de normalizar los datasets.\n",
    "\n",
    "Dataset balanceado con SMOTE:\n",
    "\n",
    "![Gráfica con datos SMOTE](Resultados/SmoteSVM.png)\n",
    "\n",
    "El modelo smote sigue siendo el más eficaz, teniendo un porcentaje de aciertos incluso mayor que la red neuronal (alrededor del 98%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión {-}\n",
    "\n",
    "Como hemos podido observar, las predicciones son mejores cuando los datos de aprendizaje son abundantes, normalizados y bien equilibrados. Cuando hay muy pocos datos, los resultados son menos precisos, y cuando no están equilibrados, están muy sesgados a favor del elemento mayoritario en los datos de aprendizaje.\n",
    "\n",
    "También podemos observar que la técnica más precisa en general ha sido SVM (aunque era la más dependiente de una buena normalización), seguida de redes neuronales y finalmente regresión logística."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62b8e38797574a9de3c3f9bc79729bb88e3b9c6776c95587f79be5c56a9bdc90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
