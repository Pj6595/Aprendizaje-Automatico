{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto final: Predicción de bancarrota {-}\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción y preparación inicial {-}\n",
    "Para este proyecto hemos usado un set de datos que contiene distintas características de miles de empresas acompañadas de una columna que indica si esas empresas acabaron o no en bancarrota.\n",
    "\n",
    "Los datos en cuestión han sido extraídos de https://www.kaggle.com/fedesoriano/company-bankruptcy-prediction\n",
    "\n",
    "Sin embargo, este dataset tiene una peculiaridad: solo cuenta con 220 ejemplos de empresas caídas en bancarrota y unos 6600 que no lo hicieron, por lo que está muy desiquilibrado. Esto puede supone un problema bastante grande para aplicar métodos de aprendizaje automático. Es por ello que hemos recurrido a la técnica SMOTE (Synthetic Minority Oversampling Technique) para poder hacer un estudio más certero de los datos.\n",
    "\n",
    "En este proyecto aplicaremos tres métodos de aprendizaje automático: regresión logística multiclase, redes neuronales y SVM, y compararemos los resultados obtenidos para el dataset original, un dataset equilibrado manualmente (eliminando gran parte de los ejemplos mayoritarios para que haya igualdad en número) y un dataset equilibrado mediante SMOTE.\n",
    "\n",
    "Para programar el proyecto se han utilizado las siguientes librerías y métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.parsers import read_csv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.optimize as opt\n",
    "import operator\n",
    "import checkNNGradients as cnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.special as special\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran tres gráficas que muestran la distribución de los datos en cada dataset que hemos utilizado. Los datos representados en verde son aquellos de empresas que no cayeron en bancarrota, y los rojos son los datos de empresas que sí lo hicieron.\n",
    "\n",
    "Esta es la gráfica de los datos originales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos originales](imagenes/DataAllExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la gráfica con los datos equilibrados a mano:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos originales](imagenes/DataReducedExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último la gráfica con los datos equilibrados con SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gráfica con datos SMOTE](imagenes/DataSMOTEBalanced.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrado de datos con SMOTE\n",
    "\n",
    "Hemos hecho uso de la librería imblearn para aplicar SMOTE a nuestro dataset. Hemos inicializado un obeto sm y luego lo hemos usado para equilibrar el tamaño de las muestras en X e y. Antes de eso hemos tenido que normalizar los datos, para lo cual hemos tenido que separar las columnas de números que están entre 0 y 1 y las que no, para que los números resultantes no estén también desequilibrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fraction_valued_columns(df):\n",
    "    my_columns  = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].max()<=1) & (df[col].min() >= 0):\n",
    "            my_columns.append(col)\n",
    "    return(my_columns)\n",
    "\n",
    "fractional_columns = get_fraction_valued_columns(df=df.drop([cname],axis=1))\n",
    "non_fractional_columns = df.drop([cname],axis=1).columns.difference(fractional_columns)\n",
    "norm = preprocessing.normalize(df[non_fractional_columns])\n",
    "normalized_df = pd.DataFrame(norm, columns=df[non_fractional_columns].columns)\n",
    "\n",
    "scaled_data = pd.concat([df.drop(non_fractional_columns,axis=1),normalized_df],axis = 1)\n",
    "\n",
    "X = scaled_data.drop(cname, axis = 1)\n",
    "y = scaled_data[cname]\n",
    "\n",
    "sm = SMOTE(random_state=123)\n",
    "X_sm , y_sm = sm.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística multiclase {-}\n",
    "\n",
    "Para aplicar este método hemos hecho uso del siguiente método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test, fileName):\n",
    "\n",
    "\tlambdas = np.linspace(1, 3, 20)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor i in lambdas:\n",
    "\t\tclasificadores = oneVsAll(X_train, y_train, 2, i)\n",
    "\t\ty_pred, acc = prediccion(X_test, y_test, clasificadores)\n",
    "\t\taccuracy.append(acc)\n",
    "\t\tprint(acc)\n",
    "\t\tcm = confusion_matrix(y_test, y_pred)\n",
    "\t\tplt.figure()\n",
    "\t\tfig = sns.heatmap(cm, annot=True,fmt=\"\",cmap='Blues').get_figure()\n",
    "\t\tfig.savefig(fileName + \"iter\" + str(i) + \".png\", dpi=400)\n",
    "\n",
    "\tplt.figure()\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"lambdaAccuracyLogistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que utiliza los siguientes métodos auxiliares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_etiquetas, reg):\n",
    "\tclasificadores = np.zeros(shape=(num_etiquetas, X.shape[1]))\n",
    "\t\n",
    "\tfor i in range(num_etiquetas):\n",
    "\t\tfiltrados = (y==i) * 1\n",
    "\t\tthetas = np.zeros(np.shape(X)[1])\n",
    "\t\tclasificadores[i] = opt.fmin_tnc(func=costeReg, x0=thetas, fprime=gradienteReg, args=(X, filtrados, reg), messages=0)[0]\n",
    "\t\t\n",
    "\treturn clasificadores\n",
    "\n",
    "def prediccion(X, Y, clasificadores):\n",
    "\tpredicciones = {}\n",
    "\tY_pred = []\n",
    "\tfor imagen in range(np.shape(X)[0]):\n",
    "\t\tfor i in range(clasificadores.shape[0]):\n",
    "\t\t\ttheta_opt = clasificadores[i]\n",
    "\t\t\tprediccion = sigmoide(\n",
    "\t\t\t\tnp.matmul(np.transpose(theta_opt), X[imagen]))\n",
    "\n",
    "\t\t\tpredicciones[i] = prediccion\n",
    "\t\tY_pred.append(max(predicciones.items(), key=operator.itemgetter(1))[0])\n",
    "\treturn Y_pred, np.sum((Y == np.array(Y_pred)))/np.shape(X)[0] * 100\n",
    "\n",
    "def costeReg(thetas, x, y, lamb):\n",
    "\tsigXT = sigmoide(np.dot(x, thetas))\n",
    "\ta1 = (-1/np.shape(x)[0])\n",
    "\ta2 = np.dot(np.log(sigXT), y)\n",
    "\ta3 = np.dot(np.log(1-sigXT+1e-6), 1-y)\n",
    "\ta = a1 * (a2+a3)\n",
    "\tb = ((lamb/(2*np.shape(x)[0])) * np.sum(thetas ** 2))\n",
    "\treturn a + b\n",
    "\n",
    "def gradienteReg(thetas, x, y, lamb):\n",
    "\tsigXT = sigmoide(np.matmul(x, thetas))\n",
    "\ta = ((1/np.shape(x)[0]) * np.matmul(np.transpose(x), (sigXT - y))) + ((lamb/np.shape(x)[0]) * thetas)\n",
    "\treturn a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son los siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí va la imagen: OriginalLamdaAccuracyLogistic\n",
    "Aquí va la imagen: OriginalIterationAccuracyLogistic\n",
    "Aquí va la imagen: Original (el heatmap)\n",
    "\n",
    "Aquí va la imagen: CutLamdaAccuracyLogistic\n",
    "Aquí va la imagen: CutIterationAccuracyLogistic\n",
    "Aquí va la imagen: Cut (el heatmap)\n",
    "\n",
    "Aquí va la imagen: SmoteLamdaAccuracyLogistic\n",
    "Aquí va la imagen: SmoteIterationAccuracyLogistic\n",
    "Aquí va la imagen: Smote (el heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales {-}\n",
    "\n",
    "Hemos aplicado el método de la red neuronal con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, fileName):\n",
    "\n",
    "\tm = len(y)\n",
    "\tinput_size = X.shape[1]\n",
    "\tnum_labels = 10\n",
    "\tnum_ocultas = 25\n",
    "\n",
    "\ty_onehot = np.zeros((m, num_labels))\n",
    "\n",
    "\tfor i in range(m):\n",
    "\t\ty_onehot[i][int(y[i])] = 1\n",
    "\n",
    "\ttheta1 = random_weights(num_ocultas, input_size + 1)\n",
    "\ttheta2 = random_weights(num_labels, num_ocultas +1)\n",
    "\tparams_rn = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "\treg_param = 1\n",
    "\n",
    "\tcost, grad = backprop(params_rn, input_size, num_ocultas,\n",
    "\t\t\t\t\t\tnum_labels, X, y_onehot, reg_param)\n",
    "\ta = cnn.checkNNGradients(backprop, 1)\n",
    "\tprint(a)\n",
    "\n",
    "\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "\ttheta1_opt = np.reshape(\n",
    "\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\ttheta2_opt = np.reshape(\n",
    "\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\n",
    "\tprint(\"El porcentaje de acierto del modelo es: {}%\".format(\n",
    "\t\tnp.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "\tlambdas = np.linspace(0, 1, 10)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor lamb in lambdas:\n",
    "\t\tprint(\"Voy por \", lamb, \" de \", len(lambdas))\n",
    "\t\treg_param = lamb\n",
    "\t\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "\t\ttheta1_opt = np.reshape(\n",
    "\t\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\t\ttheta2_opt = np.reshape(\n",
    "\t\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\t\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\t\taccuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"lambdaAccuracy\")\n",
    "\n",
    "\tlambdas = np.linspace(10, 70, 7)\n",
    "\n",
    "\taccuracy = []\n",
    "\n",
    "\tfor lamb in lambdas:\n",
    "\t\treg_param = lamb\n",
    "\t\tfmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels,\n",
    "\t\t\t\t\t\t\tX, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': int(lamb)})\n",
    "\n",
    "\t\ttheta1_opt = np.reshape(\n",
    "\t\t\tfmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "\t\ttheta2_opt = np.reshape(\n",
    "\t\t\tfmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "\t\ta1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\t\taccuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\tplt.figure()\n",
    "\tplt.plot(lambdas, accuracy)\n",
    "\tplt.savefig(fileName + \"iterationAccuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que utiliza los siguientes métodos auxiliares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_prima(x):\n",
    "\treturn sigmoide(x) / (1 - sigmoide(x))\n",
    "\n",
    "def propagar(X, theta1, theta2):\n",
    "\tm = np.shape(X)[0]\n",
    "\n",
    "\ta1 = np.hstack([np.ones([m, 1]), X])\n",
    "\tz2 = np.dot(a1, theta1.T)\n",
    "\ta2 = np.hstack([np.ones([m, 1]), sigmoide(z2)])\n",
    "\tz3 = np.dot(a2, theta2.T)\n",
    "\ta3 = sigmoide(z3)\n",
    "\treturn a1, a2, a3\n",
    "\n",
    "\n",
    "def coste_neuronal(X, y, theta1, theta2, reg):\n",
    "\ta1, a2, h = propagar(X, theta1, theta2)\n",
    "\tm = X.shape[0]\n",
    "\n",
    "\tJ = 0\n",
    "\tfor i in range(m):\n",
    "\t\tJ += np.sum(-y[i]*np.log(h[i]) - (1 - y[i])*np.log(1-h[i]+1e-9))\n",
    "\tJ = J/m\n",
    "\n",
    "\tsum_theta1 = np.sum(np.square(theta1[:, 1:]))\n",
    "\tsum_theta2 = np.sum(np.square(theta2[:, 1:]))\n",
    "\n",
    "\treg_term = (sum_theta1 + sum_theta2) * reg / (2*m)\n",
    "\n",
    "\treturn J + reg_term\n",
    "\n",
    "\n",
    "def gradiente(X, y, Theta1, Theta2, reg):\n",
    "\tm = X.shape[0]\n",
    "\n",
    "\tdelta1 = np.zeros(Theta1.shape)\n",
    "\tdelta2 = np.zeros(Theta2.shape)\n",
    "\n",
    "\ta1, a2, h = propagar(X, Theta1, Theta2)\n",
    "\n",
    "\tfor t in range(m):\n",
    "\t\ta1t = a1[t, :]\n",
    "\t\ta2t = a2[t, :]\n",
    "\t\tht = h[t, :]\n",
    "\t\tyt = y[t]\n",
    "\t\td3t = ht - yt\n",
    "\t\td2t = np.dot(Theta2.T, d3t) * (a2t * (1 - a2t))\n",
    "\n",
    "\t\tdelta1 = delta1 + np.dot(d2t[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "\t\tdelta1[:, 1:] += Theta1[:, 1:] * reg/m\n",
    "\t\tdelta2 = delta2 + np.dot(d3t[:, np.newaxis], a2t[np.newaxis, :])\n",
    "\t\tdelta2[:, 1:] += Theta2[:, 1:] * reg/m\n",
    "\n",
    "\treturn np.concatenate((np.ravel(delta1/m), np.ravel(delta2/m)))\n",
    "\n",
    "\n",
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "\tTheta1 = np.reshape(\n",
    "\t\tparams_rn[:num_ocultas * (num_entradas + 1)], (num_ocultas, (num_entradas + 1)))\n",
    "\tTheta2 = np.reshape(\n",
    "\t\tparams_rn[num_ocultas * (num_entradas + 1):], (num_etiquetas, (num_ocultas+1)))\n",
    "\treturn coste_neuronal(X, y, Theta1, Theta2, reg), gradiente(X, y, Theta1, Theta2, reg)\n",
    "\n",
    "\n",
    "def random_weights(L_in, L_out):\n",
    "\tepsilon = np.sqrt(6)/np.sqrt(L_in + L_out)\n",
    "\treturn np.random.random((L_in, L_out)) * epsilon - epsilon/2\n",
    "\n",
    "\n",
    "def predict_nn(X, h):\n",
    "\treturn [(np.argmax(h[image])) for image in range(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine {-}\n",
    "\n",
    "Para aplicar la técnica de las SVM, hemos hecho uso del siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X, y, fileName):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) \n",
    "\t\n",
    "\tvalues = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]\n",
    "\tn = len(values)\n",
    "\tscores = np.zeros((n, n))\n",
    "\n",
    "\tstartTime = time.process_time()\n",
    "\n",
    "\tfor i in range(n):\n",
    "\t\tC = values[i]\n",
    "\t\tprint(\"Voy por el i \", i, \"de \" , n) \n",
    "\t\tfor j in range(n):\n",
    "\t\t\tprint(\"Voy por el j \", j, \"de \" , n) \n",
    "\t\t\tsigma = values[j]\n",
    "\t\t\tsvm = SVC(kernel='rbf', C = C, gamma= 1 / (2 * sigma **2))\n",
    "\t\t\tsvm.fit(X_train, y_train)\n",
    "\t\t\tscores[i, j] = svm.score(X_val, y_val)\n",
    "\n",
    "\tprint(\"Error mínimo: {}\".format(1 - scores.max())) \n",
    "\tC_opt = values[scores.argmax()//n]\n",
    "\tsigma_opt = values[scores.argmax()%n]\n",
    "\tprint(\"C óptimo: {}, sigma óptimo: {}\".format(C_opt, sigma_opt))\n",
    "\n",
    "\tsvm = SVC(kernel='rbf', C= C_opt, gamma=1 / (2 * sigma_opt)**2)\n",
    "\tsvm.fit(X_train, y_train)\n",
    "\ty_pred = svm.predict(X_test)\n",
    "\tendTime = time.process_time()\n",
    "\n",
    "\tscore = svm.score(X_test, y_test)\n",
    "\ttotalTime = endTime - startTime\n",
    "\n",
    "\tprint('Precisión: {:.3f}%'.format(score*100))\n",
    "\tprint('Tiempo de ejecución: {}'.format(totalTime))\n",
    "\tprint('Matriz de confusión: ')\n",
    "\tcm = confusion_matrix(y_test, y_pred)\n",
    "\tfig = sns.heatmap(cm, annot=True,fmt=\"\",cmap='Blues').get_figure()\n",
    "\tfig.savefig(fileName, dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión {-}\n",
    "\n",
    "Como hemos podido observar, las predicciones son mejores cuando los datos de aprendizaje son abundantes y bien equilibrados. Cuando hay muy pocos datos, los resultados son menos precisos, y cuando no están equilibrados, están muy sesgados a favor del elemento mayoritario en los datos de aprendizaje."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62b8e38797574a9de3c3f9bc79729bb88e3b9c6776c95587f79be5c56a9bdc90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
