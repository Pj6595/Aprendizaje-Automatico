{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5 - Regresión lineal regularizada: sesgo y varianza {-}\r\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación inicial {-}\r\n",
    "A continuación se importan todas las librerías externas que serán utilizadas en esta práctica y se cargan los datos para hacer las distintas matrices sobre las que se va a trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from numpy.lib.function_base import gradient\r\n",
    "import operator\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from numpy.lib.npyio import load\r\n",
    "import scipy.optimize as opt\r\n",
    "from scipy.io import loadmat\r\n",
    "\r\n",
    "X = data['X']\r\n",
    "X_orig = X\r\n",
    "y = data['y']\r\n",
    "\r\n",
    "X_test = data['Xtest']\r\n",
    "X_test_orig = X_test\r\n",
    "y_test = data['ytest']\r\n",
    "\r\n",
    "X_val = data['Xval']\r\n",
    "X_val_orig = X_val\r\n",
    "y_val = data['yval']\r\n",
    "\r\n",
    "X = np.hstack([np.ones([X.shape[0],1]),X])\r\n",
    "X_val=np.hstack([np.ones([X_val.shape[0],1]),X_val])\r\n",
    "X_test=np.hstack([np.ones([X_test.shape[0],1]),X_test])\r\n",
    "\r\n",
    "thetas = np.ones(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos durante toda la práctica las siguientes funciones para calcular el coste y el gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(thetas, X, Y, reg=0):\r\n",
    "    m = X.shape[0]\r\n",
    "    H = np.dot(X, thetas)\r\n",
    "    cost = (1/(2*m)) * np.sum(np.square((H-Y.T))) + (reg/(2*m)) * np.sum(np.square(thetas[1:]))\r\n",
    "    return cost\r\n",
    "\r\n",
    "def gradient(thetas, X, Y, reg=0):\r\n",
    "    aux = np.hstack(([0], thetas[1:]))\r\n",
    "    m = X.shape[0]\r\n",
    "    H = np.dot(X, thetas)\r\n",
    "    grad = (1/m) * np.dot((H-Y.T), X) + (reg/m) * aux\r\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Regresión lineal regularizada {-}\r\n",
    "Hemos utilizado la función scipy.optimize.minimize para encontrar el valor de $\\theta$ que minimiza el error sobre los ejemplos de entrenamiento y posteriormente lo hemos dibujado sobre una gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_1():\r\n",
    "    print(\"Cost: \" + str(cost(thetas, X, y, 1)))\r\n",
    "    print(\"Gradient: \" + str(gradient(thetas, X, y, 1)))\r\n",
    "\r\n",
    "    reg = 0\r\n",
    "    thetas_opt = opt.minimize(fun= cost, x0= thetas, args= (X, y, reg)).x\r\n",
    "\r\n",
    "    plt.figure()\r\n",
    "    plt.scatter(X[:,1], y, marker= \"x\", color=\"red\")\r\n",
    "    Y_pred = np.dot(X, thetas_opt)\r\n",
    "    plt.plot(X[:,1], Y_pred)\r\n",
    "    plt.xlabel('Change in water level (x)')\r\n",
    "    plt.ylabel('Water flowing out of the dam (y)')\r\n",
    "    plt.savefig(\"apartado1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación inicial de la recta de regresión](apartado1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Curvas de aprendizaje {-}\r\n",
    "Mediante la función get_errors se ha repetido la operación del apartado anterior utilizando diferentes subconjuntos de los ejemplos de entrenamiento (de 1 a m, siendo los subconjuntos X[:i] e y[:i]), tras lo cual se han obtenido los errores para los distintos conjuntos de entrenamiento y validación. Después se han representado gráficamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_2():\r\n",
    "    m = X.shape[0]\r\n",
    "    reg = 0\r\n",
    "    \r\n",
    "    train_errors, val_errors = get_errors(X, y, X_val, y_val, reg)\r\n",
    "\r\n",
    "    plt.figure()\r\n",
    "    plt.plot(range(1, m+1), train_errors)\r\n",
    "    plt.plot(range(1, m+1), val_errors, c='orange')\r\n",
    "    plt.legend((\"Train\", \"Cross Validation\"))\r\n",
    "    plt.xlabel(\"Number of training examples\")\r\n",
    "    plt.ylabel(\"Error\")\r\n",
    "    plt.savefig(\"apartado2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación de la curva de aprendizaje para la regresión lineal](apartado2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, el error se aproxima a los conjuntos de entrenamiento cuantos más ejemplos hay, por lo que se puede concluir que el aprendizaje está sesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Regresión Polinomial {-}\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1abe66e4d7512be242466e0d397234457d143a3c6736367abb31ed87316062f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('python': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}