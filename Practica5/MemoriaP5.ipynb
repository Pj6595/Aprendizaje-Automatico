{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5 - Regresión lineal regularizada: sesgo y varianza {-}\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación inicial {-}\n",
    "A continuación se importan todas las librerías externas que serán utilizadas en esta práctica y se cargan los datos para hacer las distintas matrices sobre las que se va a trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.function_base import gradient\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.npyio import load\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "X = data['X']\n",
    "X_orig = X\n",
    "y = data['y']\n",
    "\n",
    "X_test = data['Xtest']\n",
    "X_test_orig = X_test\n",
    "y_test = data['ytest']\n",
    "\n",
    "X_val = data['Xval']\n",
    "X_val_orig = X_val\n",
    "y_val = data['yval']\n",
    "\n",
    "X = np.hstack([np.ones([X.shape[0],1]),X])\n",
    "X_val=np.hstack([np.ones([X_val.shape[0],1]),X_val])\n",
    "X_test=np.hstack([np.ones([X_test.shape[0],1]),X_test])\n",
    "\n",
    "thetas = np.ones(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos durante toda la práctica las siguientes funciones para calcular el coste y el gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(thetas, X, Y, reg=0):\n",
    "    m = X.shape[0]\n",
    "    H = np.dot(X, thetas)\n",
    "    cost = (1/(2*m)) * np.sum(np.square((H-Y.T))) + (reg/(2*m)) * np.sum(np.square(thetas[1:]))\n",
    "    return cost\n",
    "\n",
    "def gradient(thetas, X, Y, reg=0):\n",
    "    aux = np.hstack(([0], thetas[1:]))\n",
    "    m = X.shape[0]\n",
    "    H = np.dot(X, thetas)\n",
    "    grad = (1/m) * np.dot((H-Y.T), X) + (reg/m) * aux\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Regresión lineal regularizada {-}\n",
    "Hemos utilizado la función scipy.optimize.minimize para encontrar el valor de $\\theta$ que minimiza el error sobre los ejemplos de entrenamiento y posteriormente lo hemos dibujado sobre una gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_1():\n",
    "    print(\"Cost: \" + str(cost(thetas, X, y, 1)))\n",
    "    print(\"Gradient: \" + str(gradient(thetas, X, y, 1)))\n",
    "\n",
    "    reg = 0\n",
    "    thetas_opt = opt.minimize(fun= cost, x0= thetas, args= (X, y, reg)).x\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,1], y, marker= \"x\", color=\"red\")\n",
    "    Y_pred = np.dot(X, thetas_opt)\n",
    "    plt.plot(X[:,1], Y_pred)\n",
    "    plt.xlabel('Change in water level (x)')\n",
    "    plt.ylabel('Water flowing out of the dam (y)')\n",
    "    plt.savefig(\"apartado1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación inicial de la recta de regresión](apartado1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Curvas de aprendizaje {-}\n",
    "Mediante la función get_errors se ha repetido la operación del apartado anterior utilizando diferentes subconjuntos de los ejemplos de entrenamiento (de 1 a m, siendo los subconjuntos X[:i] e y[:i]), tras lo cual se han obtenido los errores para los distintos conjuntos de entrenamiento y validación. Después se han representado gráficamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_2():\n",
    "    m = X.shape[0]\n",
    "    reg = 0\n",
    "    \n",
    "    train_errors, val_errors = get_errors(X, y, X_val, y_val, reg)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, m+1), train_errors)\n",
    "    plt.plot(range(1, m+1), val_errors, c='orange')\n",
    "    plt.legend((\"Train\", \"Cross Validation\"))\n",
    "    plt.xlabel(\"Number of training examples\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.savefig(\"apartado2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación de la curva de aprendizaje para la regresión lineal](apartado2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, el error se aproxima a los conjuntos de entrenamiento cuantos más ejemplos hay, por lo que se puede concluir que el aprendizaje está sesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Regresión Polinomial {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos comenzado implementando una función que genera datos adecuados de entrada a partir de una matriz X y un número p (el grado del polinomio sobre el que se va a trabajar). A partir de dichos datos hemos vuelto a aplicar el método de regresión lineal para obtener un vector $\\theta$ que minimiza el error para un valor de $\\lambda = 0$. Finalmente hemos representado la curva en un gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_3_1():\n",
    "    p = 8\n",
    "    reg = 0\n",
    "    \n",
    "    X_pol, mu, sigma = normalizar(polinomial(X_orig, p))\n",
    "    X_pol = np.hstack([np.ones((X_pol.shape[0], 1)), X_pol])\n",
    "\n",
    "    thetas = np.zeros(X_pol.shape[1])\n",
    "\n",
    "    thetas_opt = opt.minimize(fun = cost, x0 = thetas, args = (X_pol, y, reg)).x\n",
    "    plt.figure()\n",
    "\n",
    "    X_test = np.arange(np.min(X),np.max(X),0.05)\n",
    "    X_test = polinomial(X_test,8)\n",
    "    X_test = (X_test - mu) / sigma\n",
    "    X_test =np.hstack([np.ones([X_test.shape[0],1]),X_test])\n",
    "    Y_pred = np.dot(X_test, thetas_opt)\n",
    "    plt.plot(np.arange(np.min(X),np.max(X),0.05),Y_pred)\n",
    "    plt.scatter(X_orig,y,marker=\"X\", color=\"red\")\n",
    "    plt.xlabel('Change in water level (x)')\n",
    "    plt.ylabel('Water flowing out of the dam (y)')\n",
    "    plt.savefig(\"apartado3_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación de la curva de regresión polinomial para lambda = 0](apartado3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación hemos generado las curvas de aprendizaje para la hipótesis polinomial con distintos números de conjuntos de ejemplo cada vez mayores y hemos evaluado los errores junto con los de otro conjunto independiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_3_2():\n",
    "    m = X.shape[0]\n",
    "    reg = 0\n",
    "    p = 8\n",
    "\n",
    "    X_pol, mu, sigma = normalizar(polinomial(X_orig, p))\n",
    "    X_pol = np.hstack([np.ones((X_pol.shape[0], 1)), X_pol])\n",
    "\n",
    "    X_val_pol = ((polinomial(X_val_orig, p)) - mu) / sigma\n",
    "    X_val_pol = np.hstack([np.ones((X_val_pol.shape[0], 1)), X_val_pol])\n",
    "\n",
    "    train_errors, val_errors = get_errors(X_pol, y, X_val_pol, y_val, reg)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, m+1), train_errors)\n",
    "    plt.plot(range(1, m+1), val_errors, c='orange')\n",
    "    plt.legend((\"Train\", \"Cross Validation\"))\n",
    "    plt.xlabel(\"Number of training examples\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.savefig(\"apartado3_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Representación de la curva de aprendizaje con hasta 12 conjuntos de datos](apartado3_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Selección del parámetro $\\lambda$ {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha vuelto a evaluar la hipótesis generada con los ejemplos de entrenamiento probando distintos valores de lambda λ ∈ { 0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10 }, y se ha dibujado una gráfica con los errores de los ejemplos de entrenamiento y los de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apartado_4():\n",
    "    lambdas = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "    p = 8\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    X_pol, mu, sigma = normalizar(polinomial(X_orig, p))\n",
    "    X_pol = np.hstack([np.ones((X_pol.shape[0], 1)), X_pol])\n",
    "\n",
    "    X_val_pol = ((polinomial(X_val_orig, p)) - mu) / sigma\n",
    "    X_val_pol = np.hstack([np.ones((X_val_pol.shape[0], 1)), X_val_pol])\n",
    "\n",
    "    thetas = np.zeros(p + 1)\n",
    "    i = 0\n",
    "\n",
    "    for l in lambdas:\n",
    "        thetas_opt = opt.minimize(fun=cost, x0=thetas, args=(X_pol, y, l)).x\n",
    "        train_errors.append(cost(thetas_opt, X_pol, y))\n",
    "        val_errors.append(cost(thetas_opt, X_val_pol, y_val))\n",
    "        i += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lambdas, train_errors)\n",
    "    plt.plot(lambdas, val_errors, c='orange')\n",
    "    plt.legend((\"Train\", \"Cross Validation\"))\n",
    "    plt.xlabel(\"lambdas\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.savefig(\"apartado_4.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparación del error con distintos valores de lambda](apartado_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se ha estimado el error de la hipótesis aplicándola a un nuevo vonjunto de datos con $\\lambda = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##estimación el error\n",
    "reg = 3\n",
    "X_test_pol = ((polinomial(X_test_orig, p)) - mu) / sigma\n",
    "X_test_pol = np.c_[np.ones((len(X_test_pol), 1)), X_test_pol]\n",
    "theta = np.zeros(p+1)\n",
    "\n",
    "thetas_opt = opt.minimize(fun=cost, x0=theta, args=(X_pol, y, reg)).x\n",
    "\n",
    "print('El error obtenido para lambda = {} es {}'.format(\n",
    "    reg, cost(thetas_opt, X_test_pol, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha obtenido un error de aproximadamente 3,57"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1abe66e4d7512be242466e0d397234457d143a3c6736367abb31ed87316062f0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
