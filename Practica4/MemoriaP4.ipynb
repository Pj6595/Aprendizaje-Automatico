{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b94dc42",
   "metadata": {},
   "source": [
    "# Práctica 4: Entrenamiento de redes neuronales {-}\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fcead",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preparación inicial {-}\n",
    "A continuación se importan todas las librerías externas que serán utilizadas en esta práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66914f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.function_base import gradient\n",
    "import checkNNGradients as cnn\n",
    "from displayData import displayData\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.npyio import load\n",
    "import scipy.optimize as opt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7eeef",
   "metadata": {},
   "source": [
    "### Muestra de los datos {-}\n",
    "A partir de las matrices de datos proporcionadas, se han elegido 100 elementos distintos y se han guardado en una imagen haciando uso de la función displayData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "data = loadmat('ex4data1.mat')\n",
    "theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "X = data['X']\n",
    "y = data['y'].ravel()\n",
    "\n",
    "sample = np.random.choice(X.shape[0], 100)\n",
    "imgs = displayData(X[sample, :])\n",
    "plt.savefig('numbers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24fb7d",
   "metadata": {},
   "source": [
    "![Representación de 100 ejemplos de entrenamiento aleatorios](numbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaec39",
   "metadata": {},
   "source": [
    "## 1 - Función de coste {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3340f1",
   "metadata": {},
   "source": [
    "Se ha implementado una función de coste que funciona con un término de regularización, de acuerdo con la siguiente fórmula:\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} [-y_{k}^{(i)}log((h_{\\theta}(x^{(i)}))_{k}) - (1-y_{k}^{(i)})log(1-(h_{\\theta}(x^{(i)}))_{k})] + \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\Theta^{(1)}_{j,k})^{2}+\\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\Theta_{j,k}^{(2)})^{2}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c79e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coste_neuronal(X, y, theta1, theta2, reg):\n",
    "    a1, a2, h = propagar(X, theta1, theta2)\n",
    "    m = X.shape[0]\n",
    "\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        J += np.sum(-y[i]*np.log(h[i]) - (1 - y[i])*np.log(1-h[i]))\n",
    "    J = J/m\n",
    "\n",
    "    sum_theta1 = np.sum(np.square(theta1[:, 1:]))\n",
    "    sum_theta2 = np.sum(np.square(theta2[:, 1:]))\n",
    "    \n",
    "    reg_term = (sum_theta1 + sum_theta2) * reg / (2*m)\n",
    "\n",
    "    return J + reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b299e",
   "metadata": {},
   "source": [
    "La función es similar a la que teníamos antes, solo que ahora se le ha sumado el término de regularización (los dos sumatorios + lambda partido de 2m)  \n",
    "Con lambda = 1, el coste de como resultado aproximadamente 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fd96b",
   "metadata": {},
   "source": [
    "## 2 - Función del gradiente {-}\n",
    "\n",
    "Se ha implementado una función que calcula el gradiente de una red neuronal de tres capas, añadiendo el término de regularización tal y como se hizo con el coste (excepto a la primera columna de $\\theta^{(l)}$), de acuerdo a la siguiente fórmula:  \n",
    "$\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}} J(\\Theta) = D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}$ para j = 0  \n",
    "$\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}} J(\\Theta) = D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\frac{\\lambda}{m}\\Theta^{(l)}_{ij}$ para j > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagar(X, theta1, theta2):\n",
    "    m = np.shape(X)[0]\n",
    "\n",
    "    a1 = np.hstack([np.ones([m,1]), X])\n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.hstack([np.ones([m, 1]), sigmoide(z2)])\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    a3 = sigmoide(z3)\n",
    "    return a1, a2, a3\n",
    "\n",
    "def gradiente(X, y, Theta1, Theta2, reg):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    delta1 = np.zeros(Theta1.shape)\n",
    "    delta2 = np.zeros(Theta2.shape)\n",
    "\n",
    "    a1, a2, h = propagar(X, Theta1, Theta2)\n",
    "\n",
    "    for t in range(m):\n",
    "        a1t = a1[t, :]\n",
    "        a2t = a2[t, :]\n",
    "        ht = h[t, :]\n",
    "        yt = y[t]\n",
    "        d3t = ht - yt\n",
    "        d2t = np.dot(Theta2.T, d3t) * (a2t * (1 - a2t))\n",
    "\n",
    "        delta1 = delta1 + np.dot(d2t[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "        delta1[:, 1:] += Theta1[:, 1:] * reg/m\n",
    "        delta2 = delta2 + np.dot(d3t[:, np.newaxis], a2t[np.newaxis, :])\n",
    "        delta2[:, 1:] += Theta2[:, 1:] * reg/m\n",
    "\n",
    "    return np.concatenate((np.ravel(delta1/m), np.ravel(delta2/m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52389bab",
   "metadata": {},
   "source": [
    "Con las funciones de coste y gradiente, ya podemos implementar la función de retropropagación, que viene dada por el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e565d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    Theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)], (num_ocultas, (num_entradas + 1)))\n",
    "    Theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):], (num_etiquetas, (num_ocultas+1)))\n",
    "    return coste_neuronal(X, y, Theta1, Theta2, reg), gradiente(X, y, Theta1, Theta2, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01036",
   "metadata": {},
   "source": [
    "### 2.1 - Comprobación del gradiente {-}\n",
    "\n",
    "Con las funciones que se han descrito antes, además de con otra función auxiliar para sacar matrices de pesos aleatorios, ya podemos prepararlo todo para utilizar la función checkNNGradients, que comprobará si las funciones proporcionadas funcionan correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19415c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights(L_in, L_out):\n",
    "    epsilon = np.sqrt(6)/np.sqrt(L_in + L_out)\n",
    "    return np.random.random((L_in, L_out)) * epsilon - epsilon/2\n",
    "\n",
    "weights = loadmat('ex4weights.mat')\n",
    "data = loadmat('ex4data1.mat')\n",
    "theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "X = data['X']\n",
    "y = data['y'].ravel()\n",
    "\n",
    "sample = np.random.choice(X.shape[0], 100)\n",
    "imgs = displayData(X[sample, :])\n",
    "plt.savefig('numbers')\n",
    "\n",
    "m = len(y)\n",
    "input_size = X.shape[1]\n",
    "num_labels = 10\n",
    "num_ocultas = 25\n",
    "\n",
    "y = y -1\n",
    "y_onehot = np.zeros((m, num_labels))\n",
    "\n",
    "for i in range(m):\n",
    "    y_onehot[i][y[i]] = 1\n",
    "a1, a2, h = propagar(X, theta1, theta2)\n",
    "\n",
    "#params_rn = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "theta1 = random_weights(theta1.shape[0], theta1.shape[1])\n",
    "theta2 = random_weights(theta2.shape[0], theta2.shape[1])\n",
    "params_rn = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "reg_param = 1\n",
    "\n",
    "cost, grad = backprop(params_rn, input_size, num_ocultas, num_labels, X, y_onehot, reg_param)\n",
    "a = cnn.checkNNGradients(backprop, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e58340",
   "metadata": {},
   "source": [
    "Al ejecutar checkNNGradients, el resultado devuelto está conformado por números muy pequeños (menores que $-10^{9}$), lo cual nos permite concluir que nuestras funciones son correctas al ser casi despreciable la diferencia entre nuestro resultado y la aproximación que establece la función de comprobación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375333d7",
   "metadata": {},
   "source": [
    "## 3 - Aprendizaje de los parámetros {-}\n",
    "\n",
    "Una vez realizadas todas las comprobaciones, ya podemos comenzar a entrenar la red neuronal con la función scipy.optimize.minimize y nuestra función que obtiene pesos aleatorios (con un valor de $\\epsilon$ calculado por nosotros mismos en _random_weights_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels, X, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "theta1_opt = np.reshape(fmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "theta2_opt = np.reshape(fmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "a1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "\n",
    "print(\"El porcentaje de acierto del modelo es: {}%\".format(np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe0826",
   "metadata": {},
   "source": [
    "Con unas 70 iteraciones, el porcentaje de acierto del modelo sale en torno al 93,74%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375647d8",
   "metadata": {},
   "source": [
    "A continuación se han calculado dos gráficas distintas. La primera plasma el porcentaje de acierto del modelo con distintos números de iteraciones. Se puede observar que, a mayor número de iteraciones, más eficaz de vuelve el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(10, 70, 7)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for lamb in lambdas:\n",
    "    reg_param = lamb\n",
    "    fmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels, X, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': int(lamb)})\n",
    "\n",
    "    theta1_opt = np.reshape(fmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "    theta2_opt = np.reshape(fmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "    a1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "    accuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "plt.plot(lambdas, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e2f2d",
   "metadata": {},
   "source": [
    "![Representación de la precisión del modelo con un número variable de iteraciones](accuracychart2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dd6bb",
   "metadata": {},
   "source": [
    "La segunda evalúa el porcentaje de acierto del modelo según el valor de lambda (que va desde 0 hasta 1), siempre con el mismo número de iteraciones (70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1702b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 1, 10)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for lamb in lambdas:\n",
    "    reg_param = lamb\n",
    "    fmin = opt.minimize(fun=backprop, x0=params_rn, args=(input_size, num_ocultas, num_labels, X, y_onehot, reg_param), method='TNC', jac=True, options={'maxiter': 70})\n",
    "\n",
    "    theta1_opt = np.reshape(fmin.x[:num_ocultas * (input_size + 1)], (num_ocultas, (input_size + 1)))\n",
    "    theta2_opt = np.reshape(fmin.x[num_ocultas * (input_size + 1):], (num_labels, (num_ocultas + 1)))\n",
    "\n",
    "    a1, a2, h = propagar(X, theta1_opt, theta2_opt)\n",
    "    accuracy.append((np.sum((y == predict_nn(X, h)))/X.shape[0] * 100))\n",
    "\n",
    "plt.plot(lambdas, accuracy)\n",
    "plt.savefig(\"accuracychart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510256c0",
   "metadata": {},
   "source": [
    "![Representación de la precisión del modelo con un valor variable de lambda](accuracychart.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
