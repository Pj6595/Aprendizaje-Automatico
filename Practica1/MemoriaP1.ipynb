{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Práctica 1: Regresión lineal\n",
    "Álvar Domingo Fernández y Pablo Jurado López"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Imports iniciales\n",
    "A continuación se importan todas las librerías que serán utilizadas en esta práctica:"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from pandas.io.parsers import read_csv\r\n",
    "from matplotlib import cm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.- Regresión lineal con una variable\n",
    "A partir de un fichero con dos columnas de datos, hemos aplicado la fórmula del descenso de gradiente para minimizar la función de coste."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La siguiente función sirve para cargar los datos que vamos a utilizar desde un archivo .csv:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def carga_csv(file_name):\r\n",
    "    return read_csv(file_name, header=None).to_numpy().astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestra la función que aplica la fórmula del descenso de gradiente a partir de X e Y (las dos columnas de la tabla del archivo .csv que hemos leído). Como ejemplo se ha programado para que haga 1500 iteraciones, en las que se irán actualizando las componentes Θ0 y Θ1 al mismo tiempo. Cuando termine, calculará el coste utilizando la función _coste_. Finalmente, se utilizará la librería _matplotlib_ para dibujar la recta obtenida en una gráfica."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def descenso_gradiente(X, Y):\r\n",
    "    m = len(X)\r\n",
    "    alpha = 0.01\r\n",
    "    theta_0 = theta_1 = 0\r\n",
    "    for _ in range(1500):\r\n",
    "        sum_0 = sum_1 = 0\r\n",
    "        for i in range(m):\r\n",
    "            sum_0 += (theta_0 + theta_1 * X[i]) - Y[i]\r\n",
    "            sum_1 += ((theta_0 + theta_1 * X[i]) - Y[i]) * X[i]\r\n",
    "        theta_0 = theta_0 - (alpha/m) * sum_0\r\n",
    "        theta_1 = theta_1 - (alpha/m) * sum_1\r\n",
    "    min_x = min(X)\r\n",
    "    max_x = max(X)\r\n",
    "    min_y = theta_0 + theta_1 * min_x\r\n",
    "    max_y = theta_0 + theta_1 * max_x\r\n",
    "\r\n",
    "    Coste = coste(X, Y, (theta_0, theta_1))\r\n",
    "    \r\n",
    "    # Dibujamos el resultado\r\n",
    "    plt.plot(X, Y, \"x\")\r\n",
    "    plt.plot([min_x, max_x], [min_y, max_y])\r\n",
    "    plt.savefig(\"descenso_gradiente.pdf\")\r\n",
    "\r\n",
    "    return (theta_0, theta_1), Coste"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestra el código de la función _coste_:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def coste(X, Y, Theta):\r\n",
    "    m = len(X)\r\n",
    "    sumatorio = 0\r\n",
    "    for i in range(m):\r\n",
    "        sumatorio += ((Theta[0] + Theta[1] * X[i]) - Y[i]) ** 2\r\n",
    "    return sumatorio / (2 * len(X))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Y finalmente, la gráfica obtenida en la función del descenso de gradiente:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![gráfica de la recta de regresión lineal dibujada](descenso_gradiente.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1- Visualización de la función de coste"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para visualizar la función de coste, hemos generado dos gráficas: una de superficie y otra de contorno.\r\n",
    "\r\n",
    "El primer paso ha sido procesar los datos iniciales dentro del rango de la gráfica que queremos hacer, y calcular el coste para cada punto de la gráfica."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_data(t0_range, t1_range, X, Y):\r\n",
    "    step = 0.1\r\n",
    "    Theta0 = np.arange(t0_range[0], t0_range[1], step)\r\n",
    "    Theta1 = np.arange(t1_range[0], t1_range[1], step)\r\n",
    "    Theta0, Theta1 = np.meshgrid(Theta0, Theta1)\r\n",
    "    Coste = np.empty_like(Theta0)\r\n",
    "    for ix, iy in np.ndindex(Theta0.shape):\r\n",
    "        Coste[ix, iy] = coste(X, Y, [Theta0[ix, iy], Theta1[ix, iy]])\r\n",
    "    return [Theta0, Theta1, Coste]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con la función _dibuja_coste_ dibujamos ambas gráficas haciendo uso de la librería _matplotlib_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dibuja_coste(Theta0, Theta1, Coste):\r\n",
    "    fig = plt.figure()\r\n",
    "    ax = fig.gca(projection='3d')\r\n",
    "    surf = ax.plot_surface(Theta0, Theta1, Coste,\r\n",
    "                           cmap=cm.rainbow, linewidth=0, antialiased=False)\r\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    fig2 = plt.figure()\r\n",
    "    plt.contour(Theta0, Theta1, Coste, np.logspace(-2, 3, 20))\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestran ambas gráficas generadas por la anterior función:\r\n",
    "\r\n",
    "![gráfica del coste (superficie)](CostesSuperficie.png)\r\n",
    "\r\n",
    "![gráfica del coste (contorno)](CostesContorno.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.- Regresión lineal con varias variables\r\n",
    "A partir de un fichero con tres columnas de datos, hemos aplicado la fórmula del descenso de gradiente para minimizar la función de coste. Como los rangos de las distintas variables son muy diferentes, lo primero que hemos hecho ha sido normalizar las variables mediante la siguiente función:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalizar(X):\r\n",
    "    mu = X.mean(axis=0)\r\n",
    "    sigma = X.std(axis=0)\r\n",
    "    X_norm = (X - mu) / sigma\r\n",
    "\r\n",
    "    return X_norm, mu, sigma"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función _normalizar_ devuelve la matriz que se le haya introducido con sus valores normalizados y la media y la desviación estándar de los datos.\r\n",
    "\r\n",
    "### 2.1.- Implementación vectorizada del descenso de gradiente\r\n",
    "\r\n",
    "A continuación se ha calculado el descenso de gradiente con una variante vectorizada de la función que se utilizó en el apartado 1, con el propósito de evitar iterar por cada elemento de los datos proporcionados, como haciamos en la variante iterativa:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gradiente_vec(X, Y, Theta, alpha):\r\n",
    "    NuevaTheta = Theta\r\n",
    "    m = np.shape(X)[0]\r\n",
    "    n = np.shape(X)[1]\r\n",
    "    H = np.dot(X, Theta)\r\n",
    "    Aux = (H - Y)\r\n",
    "    for i in range(n):\r\n",
    "        Aux_i = Aux * X[:, i]\r\n",
    "        NuevaTheta[i] -= (alpha / m) * Aux_i.sum()\r\n",
    "    return NuevaTheta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función _gradiente_vec_ ajusta el valor de Theta en función de los datos proporcionados y el alpha elegido.\r\n",
    "\r\n",
    "Por su parte, la función _descenso_gradiente_vec_ realiza el cálculo de _gradiente_vec_ un número determinado de iteraciones, a la vez que calcula los costes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def descenso_gradiente_vec(X, Y, alpha):\r\n",
    "    Theta = np.zeros(np.shape(X)[1])\r\n",
    "    iteraciones = 500\r\n",
    "    costes = np.zeros(iteraciones)\r\n",
    "    for i in range(iteraciones):\r\n",
    "        costes[i] = coste_vectorizado(X, Y, Theta)\r\n",
    "        Theta = gradiente_vec(X, Y, Theta, alpha)\r\n",
    "\r\n",
    "    return Theta, costes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para el cálculo de los costes también se ha implementado una función de coste vectorizada:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def coste_vectorizado(X, Y, Theta):\r\n",
    "    H = np.dot(X, Theta)\r\n",
    "    Aux = (H - Y) ** 2\r\n",
    "    return Aux.sum() / (2*len(X))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con los costes que se han calculado, se ha dibujado una gráfica con _matplotlib_ que muestra la evolución de los costes según el valor de alfa con el que se calcule la función. En _apartado_2_1_ se puede ver cómo se hace el dibujo de la gráfica, además de las preparaciones iniciales para el descenso de gradiente vectorizado (se normalizan los datos de la matriz X y se le añade una columna llena de unos para que se pueda calcular Theta como producto de matrices)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def apartado_2_1():\r\n",
    "    data = carga_csv(\"ex1data2.csv\")\r\n",
    "    X = data[:, :-1]\r\n",
    "    Y = data[:, -1]\r\n",
    "    m = np.shape(X)[0]\r\n",
    "\r\n",
    "    X, mu, sigma = normalizar(X)\r\n",
    "    X = np.hstack([np.ones([m, 1]), X])\r\n",
    "\r\n",
    "    alphas = [0.3, 0.1, 0.03, 0.01]\r\n",
    "    colors = ['indigo', 'darkviolet', 'mediumorchid', 'plum']\r\n",
    "\r\n",
    "    plt.figure()\r\n",
    "\r\n",
    "    for i in range(len(alphas)):\r\n",
    "        Theta, costes = descenso_gradiente_vec(X, Y, alphas[i])\r\n",
    "        plt.scatter(np.arange(np.shape(costes)[\r\n",
    "                    0]), costes, c=colors[i], label='alpha = ' + str(alphas[i]))\r\n",
    "\r\n",
    "    plt.legend()\r\n",
    "    plt.savefig(\"pjbobo.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestra la gráfica obtenida por esta función:\r\n",
    "\r\n",
    "![gráfica de la comparativa de costes](ComparativaCostes.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.- Ecuación normal\r\n",
    "Finalmente, calcularemos el valor óptimo de Theta mediante el uso de la ecuación normal, que nos evita tener que hacer bucles como en el método del descenso de gradiente. En este caso no hace falta normalizar los atributos. Para hacer el cálculo simplemente aplicamos la fórmula correspondiente en una línea de código:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ecuacion_normal(X, Y):\r\n",
    "    Theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(X), X)), np.transpose(X)), Y)\r\n",
    "    return Theta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez calculada, procederemos a comprobar si las predicciones hechas con ambas fórmulas son similares. Para ello calclularemos los valores de Theta y a partir de ellos calcularemos la predicción correspondiente con cada método, por ejemplo, para una casa con una superficie de 1650 metros cuadrados y 3 habitaciones:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def apartado_2_2():\r\n",
    "    data = carga_csv('ex1data2.csv')\r\n",
    "    X = data[:, :-1]\r\n",
    "    Y = data[:, -1]\r\n",
    "    m = np.shape(X)[0]\r\n",
    "\r\n",
    "    X_norm, mu, sigma = normalizar(X)\r\n",
    "    X_norm = np.hstack([np.ones([m, 1]), X_norm])\r\n",
    "\r\n",
    "    theta_vec, costecitos = descenso_gradiente_vec(X_norm, Y, 0.01)\r\n",
    "\r\n",
    "    X = np.hstack([np.ones([m, 1]), X])\r\n",
    "    theta_normal = ecuacion_normal(X, Y)\r\n",
    "\r\n",
    "    pred_normal = theta_normal[0] + \\\r\n",
    "        theta_normal[1] * 1650 + theta_normal[2] * 3\r\n",
    "    pred_gradient = theta_vec[0] + theta_vec[1] * \\\r\n",
    "        ((1650 - mu[0]) / sigma[0]) + theta_vec[2] * ((3 - (mu[1]) / sigma[1]))\r\n",
    "\r\n",
    "    print('Theta de ecuación normal: ', pred_normal, '\\n')\r\n",
    "    print('Theta de gradiente vectorizado: ', pred_gradient, '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora solo queda observar los prints que hace el programa:\r\n",
    "\r\n",
    "**Theta de ecuación normal:  293081.4643348959**\r\n",
    "\r\n",
    "**Theta de gradiente vectorizado:  299500.8939033111**\r\n",
    "\r\n",
    "Como se puede observar, los resultados son bastante similares, por lo que se ha podido llegar a la conclusión de que los cálculos realizados, tanto mediante el método de descenso de gradiente como el de la ecuación normal, son correctos."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}